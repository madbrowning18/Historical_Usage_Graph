{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac24dd4a",
   "metadata": {},
   "source": [
    "## Uploading Ask Sage Documentation into a Ask Sage Dataset\n",
    "\n",
    "This script is used to upload the Ask Sage Documentation into a Ask Sage Dataset. But is designed as an example use case. Users still need to refactor their code based on what they want to accomplish.\n",
    "\n",
    "Make sure to reach the readme file for more information on how to use this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdfd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "from asksageclient import AskSageClient\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069303b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ Ask Sage Dataset Retrieval\n",
      "================================================================================\n",
      "\n",
      "üì° Processing Ask Sage Environment Name tenant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:18:20,543 - INFO - Successfully created client for Ask Sage Environment Name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created for Ask Sage Environment Name\n",
      "   Token: Retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:18:20,880 - INFO - Filtered 71 datasets to 1 for Ask Sage Environment Name\n",
      "2025-11-05 16:18:20,881 - INFO - Set dataset name for Ask Sage Environment Name: user_custom_34125_example-testing-dataset-script_content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 matching dataset(s)\n",
      "   Dataset name: user_custom_34125_example-testing-dataset-script_content\n",
      "\n",
      "================================================================================\n",
      "üìä DATASETS TABLE\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>user_custom_34125_example-testing-dataset-scri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name\n",
       "70  user_custom_34125_example-testing-dataset-scri..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Summary:\n",
      "   Tenant: Ask Sage Environment Name\n",
      "   Total datasets found: 1\n",
      "   Dataset name: user_custom_34125_example-testing-dataset-script_content\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TenantInstance:\n",
    "    \"\"\"Container for tenant client, token, and metadata\"\"\"\n",
    "    name: str\n",
    "    client: AskSageClient\n",
    "    token: str\n",
    "    api_key: str\n",
    "    user_base_url: Optional[str] = None\n",
    "    server_base_url: Optional[str] = None\n",
    "    datasets_count: int = 0\n",
    "    dataset_name: Optional[str] = None\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Custom representation hiding sensitive data\"\"\"\n",
    "        return (f\"TenantInstance(name='{self.name}', \"\n",
    "                f\"datasets_count={self.datasets_count}, \"\n",
    "                f\"dataset_name='{self.dataset_name}', \"\n",
    "                f\"token={'***' if self.token else 'None'})\")\n",
    "\n",
    "# Tenant configuration - needs to be modified depending on the tenant you are working with.\n",
    "TENANT_NAME = \"Ask Sage Environment Name\"\n",
    "API_KEY_ENV = \"API_KEY\"\n",
    "USER_BASE_URL = \"https://api.asksage.ai/user/\"\n",
    "SERVER_BASE_URL = \"https://api.asksage.ai/server/\"\n",
    "\n",
    "# Default filter string for datasets - assumes that dataset has already been created this script just retrieves it and then uploads documents to it.\n",
    "DEFAULT_FILTER_STRING = '_example-testing-dataset-script_'\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_tenant_instance(email: str) -> TenantInstance:\n",
    "    \"\"\"\n",
    "    Create a TenantInstance with client and token\n",
    "    \n",
    "    Args:\n",
    "        email: User email address\n",
    "        \n",
    "    Returns:\n",
    "        TenantInstance object\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If email is invalid or API key is missing\n",
    "    \"\"\"\n",
    "    if not email or '@' not in email:\n",
    "        raise ValueError(f\"Invalid email address: {email}\")\n",
    "    \n",
    "    api_key = os.getenv(API_KEY_ENV)\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(f\"{API_KEY_ENV} not found in environment variables\")\n",
    "    \n",
    "    try:\n",
    "        client = AskSageClient(\n",
    "            email=email,\n",
    "            api_key=api_key,\n",
    "            user_base_url=USER_BASE_URL,\n",
    "            server_base_url=SERVER_BASE_URL\n",
    "        )\n",
    "        \n",
    "        token = client.headers.get('x-access-tokens', '')\n",
    "        \n",
    "        if not token:\n",
    "            logger.warning(f\"No token found in headers for {TENANT_NAME}\")\n",
    "        \n",
    "        instance = TenantInstance(\n",
    "            name=TENANT_NAME,\n",
    "            client=client,\n",
    "            token=token,\n",
    "            api_key=api_key,\n",
    "            user_base_url=USER_BASE_URL,\n",
    "            server_base_url=SERVER_BASE_URL\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully created client for {TENANT_NAME}\")\n",
    "        return instance\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating client for {TENANT_NAME}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_filtered_datasets(\n",
    "    instance: TenantInstance, \n",
    "    filter_string: str = DEFAULT_FILTER_STRING\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve and filter datasets from the tenant instance\n",
    "    \n",
    "    Args:\n",
    "        instance: TenantInstance object\n",
    "        filter_string: String to filter dataset names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with filtered datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datasets = instance.client.get_datasets()\n",
    "        \n",
    "        if not datasets:\n",
    "            logger.info(f\"No datasets found for {instance.name}\")\n",
    "            return pd.DataFrame(columns=['id', 'name'])\n",
    "        \n",
    "        df = pd.DataFrame(datasets)\n",
    "        \n",
    "        # Drop status column if it exists\n",
    "        if 'status' in df.columns:\n",
    "            df = df.drop(columns=['status'])\n",
    "        \n",
    "        # Rename response to name if needed\n",
    "        if 'response' in df.columns:\n",
    "            df = df.rename(columns={'response': 'name'})\n",
    "        \n",
    "        # Filter datasets by name\n",
    "        if filter_string and 'name' in df.columns:\n",
    "            original_count = len(df)\n",
    "            df = df[df['name'].str.contains(filter_string, case=False, na=False)]\n",
    "            logger.info(f\"Filtered {original_count} datasets to {len(df)} for {instance.name}\")\n",
    "        \n",
    "        # Update instance metadata\n",
    "        instance.datasets_count = len(df)\n",
    "        \n",
    "        if not df.empty and 'name' in df.columns:\n",
    "            instance.dataset_name = df.iloc[0]['name']\n",
    "            logger.info(f\"Set dataset name for {instance.name}: {instance.dataset_name}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving datasets for {instance.name}: {str(e)}\", exc_info=True)\n",
    "        return pd.DataFrame(columns=['id', 'name'])\n",
    "\n",
    "\n",
    "def print_summary(instance: TenantInstance, datasets_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print summary statistics\n",
    "    \n",
    "    Args:\n",
    "        instance: TenantInstance object\n",
    "        datasets_df: DataFrame with datasets\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Tenant: {instance.name}\")\n",
    "    print(f\"   Total datasets found: {len(datasets_df)}\")\n",
    "    print(f\"   Dataset name: {instance.dataset_name or 'N/A'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main() -> Tuple[TenantInstance, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Main execution function for tenant initialization\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - TenantInstance object\n",
    "        - DataFrame with filtered datasets\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If EMAIL environment variable is not set\n",
    "    \"\"\"\n",
    "    \n",
    "    load_dotenv(dotenv_path='.env')\n",
    "    EMAIL = os.getenv(\"EMAIL\")\n",
    "    \n",
    "    if not EMAIL:\n",
    "        raise ValueError(\"EMAIL not found in environment variables. Please set it in .env file\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ Ask Sage Dataset Retrieval\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüì° Processing {TENANT_NAME} tenant...\")\n",
    "    \n",
    "    # Create tenant instance\n",
    "    instance = create_tenant_instance(EMAIL)\n",
    "    \n",
    "    print(f\"‚úÖ Client created for {TENANT_NAME}\")\n",
    "    print(f\"   Token: {'Retrieved' if instance.token else 'Missing'}\")\n",
    "    \n",
    "    # Get filtered datasets\n",
    "    datasets_df = get_filtered_datasets(instance)\n",
    "    \n",
    "    if not datasets_df.empty:\n",
    "        print(f\"‚úÖ Found {len(datasets_df)} matching dataset(s)\")\n",
    "        print(f\"   Dataset name: {instance.dataset_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä DATASETS TABLE\")\n",
    "        print(\"=\" * 80)\n",
    "        display(datasets_df)\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  No matching datasets found\")\n",
    "    \n",
    "    print_summary(instance, datasets_df)\n",
    "    \n",
    "    return instance, datasets_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS FOR ACCESSING STORED DATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_client(instance: TenantInstance) -> AskSageClient:\n",
    "    \"\"\"Retrieve the tenant's client\"\"\"\n",
    "    return instance.client\n",
    "\n",
    "\n",
    "def get_token(instance: TenantInstance) -> str:\n",
    "    \"\"\"Retrieve the tenant's token\"\"\"\n",
    "    return instance.token\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        tenant_instance, datasets_df = main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìö DOCUMENT INGESTION SYSTEM\n",
      "   Supported formats: Word (.docx, .doc), PDF (.pdf), PowerPoint (.pptx, .ppt)\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset: user_custom_34125_example-testing-dataset-script_content\n",
      "‚úì Loaded ingestion log with 5 records\n",
      "\n",
      "üìÇ Scanning local files...\n",
      "‚úì Found 5 document files\n",
      "   File type breakdown:\n",
      "   - .docx: 3 files\n",
      "   - .pdf: 2 files\n",
      "\n",
      "üîß Upload limit set to: ALL files\n",
      "üîç Fetching ingested files from API...\n",
      "‚úì Found 83 files in API\n",
      "\n",
      "üìä ANALYSIS RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üÜï New Files</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üìù Modified Files</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚úì Unchanged Files</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üóëÔ∏è  Deleted Files</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category  Count\n",
       "0        üÜï New Files      0\n",
       "1   üìù Modified Files      0\n",
       "2  ‚úì Unchanged Files      5\n",
       "3  üóëÔ∏è  Deleted Files      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì All files are up to date!\n",
      "\n",
      "================================================================================\n",
      "üßπ CLEANUP\n",
      "================================================================================\n",
      "\n",
      "‚úì Kept permanent file: ingestion_log.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# INGESTION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "INGESTION_LOG_PATH = \"ingestion_log.csv\"\n",
    "DOCS_BASE_PATH = \"docs\"\n",
    "VERIFICATION_RETRIES = 3\n",
    "VERIFICATION_DELAY = 5  # seconds between verification attempts\n",
    "UPLOAD_DELAY = 10  # seconds between uploads\n",
    "DELETE_DELAY = 3  # seconds after deletion before re-upload\n",
    "\n",
    "# Temporary files to delete after script completes\n",
    "TEMP_FILES = [\"files_to_upload.csv\", \"upload_progress.csv\", \"files_to_delete.csv\"]\n",
    "\n",
    "# Upload limit configuration\n",
    "UPLOAD_LIMIT = 5  # Change to 'ALL' for production\n",
    "\n",
    "# Supported file extensions\n",
    "SUPPORTED_EXTENSIONS = {'.docx', '.doc', '.pdf', '.pptx', '.ppt'}\n",
    "\n",
    "# ============================================================================\n",
    "# INGESTION LOG MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def load_ingestion_log() -> pd.DataFrame:\n",
    "    \"\"\"Load existing ingestion log or create new one\"\"\"\n",
    "    if Path(INGESTION_LOG_PATH).exists():\n",
    "        df = pd.read_csv(INGESTION_LOG_PATH)\n",
    "        print(f\"‚úì Loaded ingestion log with {len(df)} records\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"üìù Creating new ingestion log\")\n",
    "        columns = [\n",
    "            'relative_path', 'filename', 'file_modified_date', \n",
    "            'file_hash', 'size_kb', 'file_type',\n",
    "            'ingestion_date', 'status'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(INGESTION_LOG_PATH, index=False)\n",
    "        return df\n",
    "\n",
    "\n",
    "def save_ingestion_log(df: pd.DataFrame):\n",
    "    \"\"\"Save ingestion log to CSV\"\"\"\n",
    "    df.to_csv(INGESTION_LOG_PATH, index=False)\n",
    "    print(f\"üíæ Saved ingestion log to {INGESTION_LOG_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FILE MANAGEMENT FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_file_hash(file_path: Path) -> str:\n",
    "    \"\"\"Calculate MD5 hash of file content for change detection\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "def get_document_files(base_path: str = DOCS_BASE_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Recursively find all Word, PDF, and PowerPoint files with metadata\"\"\"\n",
    "    docs_path = Path(base_path)\n",
    "    if not docs_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {docs_path}\")\n",
    "    \n",
    "    file_list = []\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        file_list.extend(docs_path.rglob(f\"*{ext}\"))\n",
    "    \n",
    "    if not file_list:\n",
    "        print(f\"‚ö†Ô∏è  No supported document files found in {docs_path}\")\n",
    "        print(f\"   Supported types: {', '.join(SUPPORTED_EXTENSIONS)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"‚úì Found {len(file_list)} document files\")\n",
    "    \n",
    "    # Count by type\n",
    "    type_counts = {}\n",
    "    for f in file_list:\n",
    "        ext = f.suffix.lower()\n",
    "        type_counts[ext] = type_counts.get(ext, 0) + 1\n",
    "    \n",
    "    print(\"   File type breakdown:\")\n",
    "    for ext, count in sorted(type_counts.items()):\n",
    "        print(f\"   - {ext}: {count} files\")\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"filename\": [f.name for f in file_list],\n",
    "        \"relative_path\": [str(f.relative_to(docs_path.parent)) for f in file_list],\n",
    "        \"file_modified_date\": [datetime.fromtimestamp(f.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S') for f in file_list],\n",
    "        \"size_kb\": [round(f.stat().st_size / 1024, 2) for f in file_list],\n",
    "        \"file_hash\": [get_file_hash(f) for f in file_list],\n",
    "        \"file_type\": [f.suffix.lower() for f in file_list],\n",
    "        \"full_path\": [str(f) for f in file_list]\n",
    "    })\n",
    "    \n",
    "    return df.sort_values(by=\"file_modified_date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Delete temporary files created during script execution\"\"\"\n",
    "    deleted_files = []\n",
    "    for temp_file in TEMP_FILES:\n",
    "        if Path(temp_file).exists():\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "                deleted_files.append(temp_file)\n",
    "                print(f\"üóëÔ∏è  Deleted temporary file: {temp_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not delete {temp_file}: {e}\")\n",
    "    \n",
    "    if deleted_files:\n",
    "        print(f\"\\n‚úì Cleaned up {len(deleted_files)} temporary file(s)\")\n",
    "    return deleted_files\n",
    "\n",
    "# ============================================================================\n",
    "# API FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_api_base_url(instance) -> str:\n",
    "    \"\"\"Get the appropriate API base URL\"\"\"\n",
    "    if instance.server_base_url:\n",
    "        return instance.server_base_url.rstrip('/')\n",
    "    return \"https://api.asksage.ai/server\"\n",
    "\n",
    "\n",
    "def fetch_api_ingested_files(instance) -> pd.DataFrame:\n",
    "    \"\"\"Fetch currently ingested files from API\"\"\"\n",
    "    print(f\"üîç Fetching ingested files from API...\")\n",
    "    url = f\"{get_api_base_url(instance)}/get-all-files-ingested\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-access-tokens\": instance.token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        files = data.get(\"response\", [])\n",
    "        \n",
    "        if files and isinstance(files, list):\n",
    "            df = pd.DataFrame(files)\n",
    "            print(f\"‚úì Found {len(df)} files in API\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No files found in API\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching from API: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def delete_file_from_dataset(filename: str, instance) -> Tuple[bool, str]:\n",
    "    \"\"\"Delete a file from the dataset using the API\"\"\"\n",
    "    if not instance.dataset_name:\n",
    "        return False, \"No dataset name configured\"\n",
    "    \n",
    "    url = f\"{get_api_base_url(instance)}/delete-filename-from-dataset\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-access-tokens\": instance.token\n",
    "    }\n",
    "    data = {\n",
    "        \"dataset\": instance.dataset_name,\n",
    "        \"filename\": filename\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return True, f\"Successfully deleted {filename} from {instance.dataset_name}\"\n",
    "        else:\n",
    "            return False, result.get('message', f'Failed to delete {filename}')\n",
    "    \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            return False, f\"File not found in dataset: {filename}\"\n",
    "        return False, f\"HTTP error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error deleting file: {str(e)}\"\n",
    "\n",
    "\n",
    "def upload_file_to_api(file_path: str, instance) -> Tuple[bool, str]:\n",
    "    \"\"\"Upload a single file to the API using the client\"\"\"\n",
    "    if not instance.dataset_name:\n",
    "        return False, \"No dataset name configured\"\n",
    "    \n",
    "    try:\n",
    "        response = instance.client.train_with_file(\n",
    "            file_path=str(file_path),\n",
    "            dataset=instance.dataset_name\n",
    "        )\n",
    "        return True, f\"Upload successful to {instance.dataset_name}: {response}\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def verify_file_ingestion(filename: str, instance, max_retries: int = VERIFICATION_RETRIES) -> bool:\n",
    "    \"\"\"Verify that a file was successfully ingested by checking the API\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        if attempt > 0:\n",
    "            print(f\"   Retry {attempt}/{max_retries-1} for {filename}...\")\n",
    "            time.sleep(VERIFICATION_DELAY)\n",
    "        \n",
    "        api_df = fetch_api_ingested_files(instance)\n",
    "        \n",
    "        if not api_df.empty:\n",
    "            if 'filename' in api_df.columns:\n",
    "                if filename in api_df['filename'].values:\n",
    "                    return True\n",
    "            elif 'name' in api_df.columns:\n",
    "                if filename in api_df['name'].values:\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def verify_file_deletion(filename: str, instance, max_retries: int = VERIFICATION_RETRIES) -> bool:\n",
    "    \"\"\"Verify that a file was successfully deleted by checking the API\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        if attempt > 0:\n",
    "            print(f\"   Retry {attempt}/{max_retries-1} verifying deletion of {filename}...\")\n",
    "            time.sleep(VERIFICATION_DELAY)\n",
    "        \n",
    "        api_df = fetch_api_ingested_files(instance)\n",
    "        \n",
    "        if api_df.empty:\n",
    "            return True\n",
    "        \n",
    "        if 'filename' in api_df.columns:\n",
    "            if filename not in api_df['filename'].values:\n",
    "                return True\n",
    "        elif 'name' in api_df.columns:\n",
    "            if filename not in api_df['name'].values:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_files(local_df: pd.DataFrame, log_df: pd.DataFrame, api_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Analyze files and categorize them for upload decisions\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'new_files': [],\n",
    "        'modified_files': [],\n",
    "        'unchanged_files': [],\n",
    "        'deleted_files': []\n",
    "    }\n",
    "    \n",
    "    log_dict = log_df.set_index('relative_path').to_dict('index') if not log_df.empty else {}\n",
    "    \n",
    "    for _, row in local_df.iterrows():\n",
    "        rel_path = row['relative_path']\n",
    "        \n",
    "        if rel_path not in log_dict:\n",
    "            results['new_files'].append(row.to_dict())\n",
    "        else:\n",
    "            log_entry = log_dict[rel_path]\n",
    "            status = log_entry.get('status')\n",
    "            \n",
    "            if row['file_hash'] != log_entry.get('file_hash'):\n",
    "                results['modified_files'].append({\n",
    "                    **row.to_dict(),\n",
    "                    'last_ingestion_date': log_entry.get('ingestion_date'),\n",
    "                    'previous_hash': log_entry.get('file_hash'),\n",
    "                    'previous_status': status\n",
    "                })\n",
    "            elif status == 'ingested':\n",
    "                results['unchanged_files'].append({\n",
    "                    **row.to_dict(),\n",
    "                    'last_ingestion_date': log_entry.get('ingestion_date')\n",
    "                })\n",
    "            else:\n",
    "                results['new_files'].append(row.to_dict())\n",
    "    \n",
    "    local_paths = set(local_df['relative_path'])\n",
    "    for rel_path, log_entry in log_dict.items():\n",
    "        if rel_path not in local_paths and log_entry.get('status') == 'ingested':\n",
    "            results['deleted_files'].append(log_entry)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def update_ingestion_log(log_df: pd.DataFrame, files_to_mark: list, status: str = 'ingested') -> pd.DataFrame:\n",
    "    \"\"\"Update ingestion log with newly ingested files\"\"\"\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    for file_info in files_to_mark:\n",
    "        rel_path = file_info['relative_path']\n",
    "        \n",
    "        if rel_path in log_df['relative_path'].values:\n",
    "            idx = log_df[log_df['relative_path'] == rel_path].index[0]\n",
    "            log_df.at[idx, 'ingestion_date'] = current_time\n",
    "            log_df.at[idx, 'status'] = status\n",
    "            log_df.at[idx, 'file_hash'] = file_info['file_hash']\n",
    "            log_df.at[idx, 'file_modified_date'] = file_info['file_modified_date']\n",
    "            log_df.at[idx, 'size_kb'] = file_info['size_kb']\n",
    "            log_df.at[idx, 'file_type'] = file_info.get('file_type', '')\n",
    "        else:\n",
    "            new_row = {\n",
    "                'relative_path': rel_path,\n",
    "                'filename': file_info['filename'],\n",
    "                'file_modified_date': file_info['file_modified_date'],\n",
    "                'file_hash': file_info['file_hash'],\n",
    "                'size_kb': file_info['size_kb'],\n",
    "                'file_type': file_info.get('file_type', ''),\n",
    "                'ingestion_date': current_time,\n",
    "                'status': status\n",
    "            }\n",
    "            log_df = pd.concat([log_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    return log_df\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH OPERATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def batch_delete_files(files_to_delete: List[Dict], instance, verify: bool = True) -> Dict:\n",
    "    \"\"\"Delete multiple files from the dataset\"\"\"\n",
    "    results = {\n",
    "        'successful': [],\n",
    "        'failed': [],\n",
    "        'verified': [],\n",
    "        'unverified': []\n",
    "    }\n",
    "    \n",
    "    total = len(files_to_delete)\n",
    "    print(f\"\\nüóëÔ∏è  Starting deletion of {total} modified files from dataset...\")\n",
    "    print(f\"   Dataset: {instance.dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, file_info in enumerate(files_to_delete, 1):\n",
    "        filename = file_info['filename']\n",
    "        file_type = file_info.get('file_type', 'unknown')\n",
    "        \n",
    "        print(f\"\\n[{idx}/{total}] Deleting: {filename}\")\n",
    "        print(f\"   Path: {file_info['relative_path']}\")\n",
    "        print(f\"   Type: {file_type}\")\n",
    "        print(f\"   Dataset: {instance.dataset_name}\")\n",
    "        print(f\"   Reason: File was modified (hash changed)\")\n",
    "        \n",
    "        delete_time = datetime.now()\n",
    "        success, message = delete_file_from_dataset(filename, instance)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úì Deletion successful\")\n",
    "            file_info['delete_time'] = delete_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            file_info['delete_response'] = message\n",
    "            results['successful'].append(file_info)\n",
    "            \n",
    "            if verify:\n",
    "                print(f\"   üîç Verifying deletion...\")\n",
    "                if verify_file_deletion(filename, instance):\n",
    "                    print(f\"   ‚úì Verified deletion from API\")\n",
    "                    results['verified'].append(file_info)\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Could not verify deletion\")\n",
    "                    results['unverified'].append(file_info)\n",
    "            \n",
    "            if idx < total:\n",
    "                print(f\"   ‚è≥ Waiting {DELETE_DELAY} seconds...\")\n",
    "                time.sleep(DELETE_DELAY)\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Deletion failed: {message}\")\n",
    "            file_info['delete_time'] = delete_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            file_info['error'] = message\n",
    "            results['failed'].append(file_info)\n",
    "            \n",
    "            if idx < total:\n",
    "                time.sleep(DELETE_DELAY)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def batch_upload_files(files_to_upload: List[Dict], instance, \n",
    "                       verify: bool = True, limit: Optional[int] = None, \n",
    "                       is_reupload: bool = False) -> Dict:\n",
    "    \"\"\"Upload multiple files to the dataset\"\"\"\n",
    "    results = {\n",
    "        'successful': [],\n",
    "        'failed': [],\n",
    "        'verified': [],\n",
    "        'unverified': [],\n",
    "        'skipped': []\n",
    "    }\n",
    "    \n",
    "    if limit is not None and limit > 0:\n",
    "        if len(files_to_upload) > limit:\n",
    "            results['skipped'] = files_to_upload[limit:]\n",
    "            files_to_upload = files_to_upload[:limit]\n",
    "            print(f\"\\n‚ö†Ô∏è  UPLOAD LIMIT: Processing only {limit} files (skipping {len(results['skipped'])})\")\n",
    "    \n",
    "    total = len(files_to_upload)\n",
    "    upload_type = \"Re-uploading\" if is_reupload else \"Uploading\"\n",
    "    print(f\"\\nüì§ Starting {upload_type.lower()} of {total} files...\")\n",
    "    print(f\"   Dataset: {instance.dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, file_info in enumerate(files_to_upload, 1):\n",
    "        file_path = file_info['full_path']\n",
    "        filename = file_info['filename']\n",
    "        file_type = file_info.get('file_type', 'unknown')\n",
    "        \n",
    "        print(f\"\\n[{idx}/{total}] {upload_type}: {filename}\")\n",
    "        print(f\"   Path: {file_info['relative_path']}\")\n",
    "        print(f\"   Type: {file_type}\")\n",
    "        print(f\"   Dataset: {instance.dataset_name}\")\n",
    "        print(f\"   Size: {file_info['size_kb']} KB\")\n",
    "        \n",
    "        upload_time = datetime.now()\n",
    "        success, message = upload_file_to_api(file_path, instance)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úì Upload successful\")\n",
    "            file_info['upload_time'] = upload_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            file_info['upload_response'] = message\n",
    "            results['successful'].append(file_info)\n",
    "            \n",
    "            if verify:\n",
    "                print(f\"   üîç Verifying ingestion...\")\n",
    "                if verify_file_ingestion(filename, instance):\n",
    "                    print(f\"   ‚úì Verified in API\")\n",
    "                    results['verified'].append(file_info)\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Could not verify in API\")\n",
    "                    results['unverified'].append(file_info)\n",
    "            \n",
    "            if idx < total:\n",
    "                print(f\"   ‚è≥ Waiting {UPLOAD_DELAY} seconds...\")\n",
    "                time.sleep(UPLOAD_DELAY)\n",
    "        else:\n",
    "            print(f\"   ‚ùå Upload failed: {message}\")\n",
    "            file_info['upload_time'] = upload_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            file_info['error'] = message\n",
    "            results['failed'].append(file_info)\n",
    "            \n",
    "            if idx < total:\n",
    "                time.sleep(UPLOAD_DELAY)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main(tenant_instance, upload_limit=UPLOAD_LIMIT):\n",
    "    \"\"\"\n",
    "    Main execution function for document ingestion\n",
    "    \n",
    "    Args:\n",
    "        tenant_instance: TenantInstance object from Code Block 1\n",
    "        upload_limit: Number of files to upload or 'ALL'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìö DOCUMENT INGESTION SYSTEM\")\n",
    "        print(\"   Supported formats: Word (.docx, .doc), PDF (.pdf), PowerPoint (.pptx, .ppt)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not tenant_instance:\n",
    "            print(\"\\n‚ùå No tenant instance provided!\")\n",
    "            print(\"   Please run Code Block 1 first to initialize the tenant.\")\n",
    "            return None, None\n",
    "        \n",
    "        if not tenant_instance.dataset_name:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: No dataset found. Skipping ingestion.\")\n",
    "            return tenant_instance, None\n",
    "        \n",
    "        print(f\"\\nüìä Dataset: {tenant_instance.dataset_name}\")\n",
    "        \n",
    "        log_df = load_ingestion_log()\n",
    "        \n",
    "        print(\"\\nüìÇ Scanning local files...\")\n",
    "        local_df = get_document_files()\n",
    "        \n",
    "        if local_df.empty:\n",
    "            print(\"\\n‚ùå No supported document files found!\")\n",
    "            print(f\"   Supported types: {', '.join(SUPPORTED_EXTENSIONS)}\")\n",
    "            return tenant_instance, log_df\n",
    "        \n",
    "        limit = None\n",
    "        if upload_limit != 'ALL':\n",
    "            try:\n",
    "                limit = int(upload_limit)\n",
    "                print(f\"\\nüîß Upload limit set to: {limit} files\")\n",
    "            except ValueError:\n",
    "                print(f\"\\n‚ö†Ô∏è  Invalid upload limit '{upload_limit}', using ALL\")\n",
    "        else:\n",
    "            print(f\"\\nüîß Upload limit set to: ALL files\")\n",
    "        \n",
    "        api_df = fetch_api_ingested_files(tenant_instance)\n",
    "        analysis = analyze_files(local_df, log_df, api_df)\n",
    "        \n",
    "        print(f\"\\nüìä ANALYSIS RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'Category': [\n",
    "                'üÜï New Files',\n",
    "                'üìù Modified Files',\n",
    "                '‚úì Unchanged Files',\n",
    "                'üóëÔ∏è  Deleted Files'\n",
    "            ],\n",
    "            'Count': [\n",
    "                len(analysis['new_files']),\n",
    "                len(analysis['modified_files']),\n",
    "                len(analysis['unchanged_files']),\n",
    "                len(analysis['deleted_files'])\n",
    "            ]\n",
    "        })\n",
    "        display(summary_df)\n",
    "        \n",
    "        files_to_upload = analysis['new_files']\n",
    "        modified_files = analysis['modified_files']\n",
    "        \n",
    "        if not files_to_upload and not modified_files:\n",
    "            print(f\"\\n‚úì All files are up to date!\")\n",
    "            return tenant_instance, log_df\n",
    "        \n",
    "        # Handle modified files (delete then re-upload)\n",
    "        if modified_files:\n",
    "            delete_results = batch_delete_files(modified_files, tenant_instance, verify=True)\n",
    "            \n",
    "            print(f\"\\nüìä DELETION RESULTS\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"‚úì Successfully Deleted: {len(delete_results['successful'])}\")\n",
    "            print(f\"‚úì Verified Deletion: {len(delete_results['verified'])}\")\n",
    "            print(f\"‚ùå Failed to Delete: {len(delete_results['failed'])}\")\n",
    "            \n",
    "            # Only re-upload files that were successfully deleted\n",
    "            if delete_results['failed']:\n",
    "                failed_filenames = {f['filename'] for f in delete_results['failed']}\n",
    "                modified_files = [f for f in modified_files if f['filename'] not in failed_filenames]\n",
    "        \n",
    "        # Upload new files\n",
    "        if files_to_upload:\n",
    "            upload_results_new = batch_upload_files(files_to_upload, tenant_instance, verify=True, \n",
    "                                                   limit=limit, is_reupload=False)\n",
    "        else:\n",
    "            upload_results_new = {'successful': [], 'failed': [], 'verified': [], 'unverified': [], 'skipped': []}\n",
    "        \n",
    "        # Re-upload modified files\n",
    "        if modified_files:\n",
    "            remaining_limit = None\n",
    "            if limit is not None:\n",
    "                remaining_limit = limit - len(upload_results_new['successful'])\n",
    "                if remaining_limit <= 0:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Upload limit reached\")\n",
    "                    modified_files = []\n",
    "            \n",
    "            if modified_files:\n",
    "                upload_results_modified = batch_upload_files(modified_files, tenant_instance, verify=True,\n",
    "                                                            limit=remaining_limit, is_reupload=True)\n",
    "        else:\n",
    "            upload_results_modified = {'successful': [], 'failed': [], 'verified': [], 'unverified': [], 'skipped': []}\n",
    "        \n",
    "        print(f\"\\nüìä FINAL UPLOAD RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        total_successful = len(upload_results_new['successful']) + len(upload_results_modified['successful'])\n",
    "        total_verified = len(upload_results_new['verified']) + len(upload_results_modified['verified'])\n",
    "        total_failed = len(upload_results_new['failed']) + len(upload_results_modified['failed'])\n",
    "        \n",
    "        print(f\"‚úì Total Uploaded: {total_successful}\")\n",
    "        print(f\"‚úì Total Verified: {total_verified}\")\n",
    "        print(f\"‚ùå Total Failed: {total_failed}\")\n",
    "        \n",
    "        # Update log with verified uploads\n",
    "        all_verified = upload_results_new['verified'] + upload_results_modified['verified']\n",
    "        if all_verified:\n",
    "            log_df = update_ingestion_log(log_df, all_verified, status='ingested')\n",
    "        \n",
    "        # Update log with unverified uploads\n",
    "        all_unverified = upload_results_new['unverified'] + upload_results_modified['unverified']\n",
    "        if all_unverified:\n",
    "            log_df = update_ingestion_log(log_df, all_unverified, status='pending_verification')\n",
    "        \n",
    "        # Update log with failed uploads\n",
    "        all_failed = upload_results_new['failed'] + upload_results_modified['failed']\n",
    "        if all_failed:\n",
    "            log_df = update_ingestion_log(log_df, all_failed, status='failed')\n",
    "        \n",
    "        save_ingestion_log(log_df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìú FINAL INGESTION LOG SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total tracked files: {len(log_df)}\")\n",
    "        print(f\"Log file location: {INGESTION_LOG_PATH}\")\n",
    "        \n",
    "        if 'status' in log_df.columns:\n",
    "            print(f\"\\nStatus Summary:\")\n",
    "            status_counts = log_df['status'].value_counts()\n",
    "            for status, count in status_counts.items():\n",
    "                if pd.notna(status):\n",
    "                    print(f\"  {status}: {count}\")\n",
    "        \n",
    "        print(f\"\\nSample of ingestion log:\")\n",
    "        display(log_df.head(10))\n",
    "        \n",
    "        return tenant_instance, log_df\n",
    "    \n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üßπ CLEANUP\")\n",
    "        print(\"=\" * 80)\n",
    "        cleanup_temp_files()\n",
    "        print(f\"\\n‚úì Kept permanent file: {INGESTION_LOG_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        \n",
    "        # tenant_instance, log_df = main(tenant_instance, upload_limit=5)\n",
    "        \n",
    "        # To run for all files:\n",
    "        tenant_instance, log_df = main(tenant_instance, upload_limit='ALL')\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"‚ùå ERROR: tenant_instance not found!\")\n",
    "        print(\"   Please run Code Block 1 first to initialize the tenant instance.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asksagetesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
